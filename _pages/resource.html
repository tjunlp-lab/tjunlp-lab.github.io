---
layout: default
title: Resource
permalink: /resource/
---
<!-- Researcher-Specific Resources -->

<!-- Researcher-Specific Resources -->

<!-- Research Resources Table -->
<br><font face="verdana" size="6" color="#309657"><strong>- Research Paper Resources</strong></font>
<hr style="margin-top:-4px;height:1px;border:none;border-top:1px dotted #309657;" />

<table border="1" width="100%" cellspacing="0" cellpadding="5" style="border-collapse: collapse;">
  <thead>
    <tr>
      <th style="font-family:verdana; font-size:16px; color:#309657;">Name</th>
      <th style="font-family:verdana; font-size:16px; color:#309657;">Year + Paper + Resource</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LIU Chuang</td>
      <td>
        (2023) M3KE: <a href="https://github.com/tjunlp-lab/M3KE">[GitHub]</a><br>
        (2024) LHMKE: <a href="https://github.com/tjunlp-lab/LHMKE">[GitHub]</a><br>
        (2024) OpenEval: <a href="http://openeval.org.cn/#/">[Website]</a>
      </td>
    </tr>
    
    <tr>
      <td>REN Yuqi</td>
      <td>
        (2021) CogAlign: <a href="https://github.com/tjunlp-lab/CogAlign">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>SHEN Tianhao</td>
      <td>
        (2024) RoleEval: <a href="https://github.com/Magnetic2014/RoleEval">[GitHub]</a><br>
        (2023) X-RiSAWOZ: <a href="https://github.com/stanford-oval/dialogues">[GitHub]</a><br>
        (2023) Large Language Model Alignment: <a href="https://github.com/Magnetic2014/llm-alignment-survey">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>JIANG Bojian</td>
      <td>
        Automated Progressive Red Teaming: <a href="https://github.com/tjunlp-lab/APRT">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>SHI Dan</td>
      <td>
        (2024) CORECODE: <a href="https://github.com/danshi777/CORECODE">[GitHub]</a><br>
        (2024) IRCAN: <a href="https://github.com/danshi777/IRCAN">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>ZHU Jingxiang</td>
      <td>
        (2024) Multilingual NMT Robustness: <a href="https://github.com/tjunlp-lab/ID-ZH-MTRobustEval">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>JIN Renren</td>
      <td>
        (2022) Multilingual NMT: <a href="https://github.com/cordercorder/nmt-multi">[GitHub]</a><br>
        (2024) LLM Quantization: <a href="https://github.com/cordercorder/quant_eval">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>LI Zhigen</td>
      <td>
        (2025) ChatSOP: <a href="https://github.com/PCA-anonymous/PCA">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>ZHANG Shaowei</td>
      <td>
        (2025) BackMATH: <a href="https://github.com/ShaoweiZhang0326/BackMath">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>DONG Weilong</td>
      <td>
        (2024) ConTrans: <a href="https://github.com/willowdong/ConTrans">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>SUN Haoran</td>
      <td>
        (2023) Multilingual E2E Speech Translation: <a href="https://github.com/Moyu-42/Towards-A-Deep-Understanding-of-Multilingual-E2E-ST">[GitHub]</a><br>
        (2024) FuxiTranyu: <a href="https://github.com/tjunlp-lab/FuxiTranyu">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>HUANG Yufei</td>
      <td>
        (2024) CBBQ Dataset: <a href="https://github.com/YFHuangxxxx/CBBQ">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>GUO Zishan</td>
      <td>
        (2023) LLM Evaluation Papers: <a href="https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers">[GitHub]</a><br>
        (2024) Chinese Spoken-to-Written: <a href="https://github.com/guozishan/CS2W">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>YU Linhao</td>
      <td>
        (2023) LFED Dataset: <a href="https://github.com/tjunlp-lab/LFED/tree/main">[GitHub]</a><br>
        (2024) CMoralEval: <a href="https://github.com/tjunlp-lab/CMoralEval">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>PAN Leiyu</td>
      <td>
        (2023) LLM Translation Robustness: <a href="https://github.com/tjunlp-lab/llm_translate_robust">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>SHI Ling</td>
      <td>
        (2024) CRiskEval: <a href="https://github.com/lingshi6565/Risk_eval">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>YANG Lei</td>
      <td>
        (2024) DCIS: <a href="https://github.com/YL-9/dcis">[GitHub]</a><br>
        (2025) ProBench: <a href="https://github.com/YL-9/probench">[GitHub]</a>
      </td>
    </tr>
    
    <tr>
      <td>XU Shaoyang</td>
      <td>
        (2024) Multilingual Human Values: <a href="https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts">[GitHub]</a><br>
        (2025) Culture Alignment: <a href="https://github.com/shaoyangxu/CultureSPA">[GitHub]</a>
      </td>
    </tr>
  </tbody>
</table>

<br>


<!-- Corpora -->
<br><font face="verdana" size="6" color=#309657><strong>- Corpora</strong></font>
<hr style="margin-top:-4px;height:1px;border:none;border-top:1px dotted #309657;" />
<ol>
  <li>
    TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models
    <ul>
      <li>paper: <a href="https://openreview.net/pdf?id=r2DdJQ9AJvI"> [pdf]</a></li>
      <li>dataset and code: <a href="https://github.com/tjunlp-lab/TGEA"><i class="fa fa-github fa-1x"></i></a> </li>
    </ul>
  </li>

  <li>
    BiPaR: A bilingual MRC dataset on novels [Jing et al. 2019]
    <ul>
      <li>paper: <a href="https://arxiv.org/abs/1910.05040?context=cs.CL"> [pdf]</a></li>
      <li>dataset: <a href="https://multinlp.github.io/BiPaR/">[homepage]</a></li>
      <li>code: <a href="https://github.com/sharejing/BiPaR"><i class="fa fa-github fa-1x"></i></a> </li>
    </ul>
  </li>

  <li>
    Dataset for Shallow Discourse Annotation for Chinese TED Talks.
    <ul>
      <li>paper: <a href="https://arxiv.org/abs/2003.04032"> [pdf]</a></li>
      <li>dataset: <a href="https://github.com/tjunlp-lab/Shallow-Discourse-Annotation-for-Chinese-TED-Talks">[homepage]</a></li>
    </ul>
  </li>
  <li>
    A Test Suite for Evaluating Discourse Phenomena in Document-level Neural Machine Translation.
    <ul>
      <li>paper: <a href="https://aclanthology.org/2020.iwdp-1.3"> [pdf]</a></li>
      <li>dataset: <a href="https://github.com/tjunlp-lab/Discourse-Phenomena-in-Document-level-Neural-Machine-Translation">[homepage]</a></li>
    </ul>
  </li>
  <li>
    RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling.
    <ul>
      <li>paper: <a href="https://arxiv.org/abs/2010.08738"> [pdf]</a></li>
      <li>dataset: <a href="https://github.com/terryqj0107/RiSAWOZ">[homepage]</a></li>
    </ul>
  </li>
  <li>
    TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks.
    <ul>
      <li>paper: <a href="https://aclanthology.org/2020.emnlp-main.223"> [pdf]</a></li>
      <li>dataset: <a href="https://github.com/wanqiulong0923/TED-CDB">[homepage]</a></li>
    </ul>
  </li>
  <li>
    Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context.
    <ul>
      <li>paper: <a href="https://aclanthology.org/2021.emnlp-main.306"> [pdf]</a></li>
      <li>dataset: <a href="https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese_WPLC">[homepage]</a></li>
    </ul>
  </li>
</ol>

<!-- Resources Section -->

<!-- Other content of the Resources section -->

<!-- Add the Research Groups table at the bottom -->
<br><br><font face="verdana" size="6" color="#309657"><strong>Research Groups at TJUNLP</strong></font>
<br />
<table border="1" cellpadding="10" cellspacing="0" style="width:100%; border-collapse: collapse;">
    <thead>
        <tr>
            <th style="padding: 8px;">Research Group</th>
            <th style="padding: 8px;">Team Leader</th>
            <th style="padding: 8px;">Team Members</th>
            <th style="padding: 8px;">Introduction</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 8px; font-weight: normal;">Data and Evaluation Group</td>
            <td style="padding: 8px; font-weight: normal;">Linhao Yu</td>
            <td style="padding: 8px; font-weight: normal;">Jiaxuan Li, Yan Liu, Yongqi Leng, Jingting Zheng, Bojian Xiong</td>
            <td style="padding: 8px; font-weight: normal;">This research group is responsible for the multi-dimensional and holistic evaluation of LLMs. The group is also involved in data synthesis work.</td>
        </tr>
        <tr>
            <td>Multilingual LLM Group</td>
            <td>Supryadi</td>
            <td>Shaolin Zhu, Leiyu Pan, Jingting Zheng, Tianyu Dong, Zhuo Chen, Nethmi Muthugala</td>
            <td>This research group focuses on the multilingual aspects of LLMs, with an emphasis on multilingual datasets, fine-tuning, alignment, and evaluation. We also explore LLM translation, cross-lingual LLMs, and LLMs for low-resource languages.</td>
        </tr>
        <tr>
            <td>Interpretability of LLM Group</td>
            <td>Xinwei Wu</td>
            <td>Bojian Jiang, Weilong Dong, Ling Shi, Zhuowen Han, Yue Chen, Zhen Wang</td>
            <td>This group focuses on researching the interpretability of LLMs (model editing, steering vectors, neuron location, sparse autoencoders, etc.) and their applications, including AI safety, privacy protection, red teaming, concept alignment, and backdoor attacks.</td>
        </tr>
        <tr>
            <td>LLM Group</td>
            <td>Renren Jin</td>
            <td>Hang Zhou, Jiangcun Dun, Linhao Yu, Leiyu Pan, Bojian Xiong, Lei Yang, Jiang Zhou, Yue Chen, Jiangyang He</td>
            <td>They work on general-domain LLM training which involves pre-training, continued pre-training, and alignment training using techniques such as supervised fine-tuning, DPO, PPO, and GRPO with various architectures, including dense and MoE models. They also focus on enhancing and processing LLM training data, utilizing LLMs for data synthesis and filtering. Reasoning optimization for LLMs, including quantization and context extension. Lastly, the group explores the training of vertical LLMs, such as code LLMs and DNA LLMs.</td>
        </tr>
        <tr>
            <td>Agent and RAG Group</td>
            <td>Dan Shi</td>
            <td>Zhigen Li, Yufei Huang, Zishan Guo, Jianxiang Peng, Juesi Xiao</td>
            <td>They work on the research and application of LLM-powered agents and RAG (Retrieval-Augmented Generation) technology. The research covers aspects such as agents' task execution and tool usage, improving the reasoning ability of intelligent agents, and enhancing their adaptability to complex tasks. They also examine agents' performance in various complex scenarios, including decision-making and execution tasks in simulations such as the United Nations Conference, international security analysis, and official document writing. Their research on RAG technology aims to combine the advantages of external knowledge bases and LLMs to enhance agents' knowledge acquisition and generation capabilities in practical applications.</td>
        </tr>
        <tr>
            <td>AI for Science Group</td>
            <td>Yuqi Ren</td>
            <td>Jiang Zhou, Xiaoyu Xiong</td>
            <td>They primarily use artificial intelligence to solve complex problems and challenges in scientific research. Currently, they are exploring DNA models and scientific discoveries based on large model intelligent agents.</td>
        </tr>
    </tbody>
</table>

